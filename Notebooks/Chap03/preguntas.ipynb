{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66961a7",
   "metadata": {},
   "source": [
    "### Preguntas del libro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ef3ab",
   "metadata": {},
   "source": [
    "**Problema 3.1**  \n",
    "¿Qué tipo de mapeo de entrada a salida se crearía si la función de activación en la ecuación 3.1 fuera lineal, de modo que $a[z] = \\psi_0 + \\psi_1 z$?  \n",
    "¿Qué tipo de mapeo se crearía si se eliminara la función de activación, es decir, si $a[z] = z$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6c7a1b",
   "metadata": {},
   "source": [
    "**Solución** \n",
    "\n",
    "La ecuación 3.1 es: \n",
    "\n",
    "$$\n",
    "f(x , \\psi)  =  \\psi_0 +  \\psi_1 a[ \\psi_{10}+  \\psi_{11}x] +  \\psi_2  a[ \\psi_{20} +  \\psi_{21}x] +  \\psi_3 a[ \\psi_{30}+ \\psi_{31}x]\n",
    "$$\n",
    "Con a[z]= RELU(Z)\n",
    "\n",
    "1. Tenemos que la ecuación 3.1  es una suma de varias funciones lineales, posibles sumas porque la función RELU  lo que hace es, darle valores en cero a valores negativos, así que la activación la podemos pensar como darle relevancian o valores positivos a lo que los parametros han configurado como positivos. Así que las función lineales que tendremos es la suma de sus partes positivas, que posiblemente, lo que se quiere es darle relevancia a distintas partes del modelo.\n",
    "Ahora, si la activación fuera lineal, tenemos que estas partes negativas, si se tendrian en cuenta, así que podrian haber activaciones en negativo, que esto podria ser darle restarle relevancia.\n",
    "\n",
    "2. Si cambiamos la función $a[z]= RELU(Z)$ por $a[z] = z$, tenemos:\n",
    "\n",
    "$$\n",
    "f(x , \\psi)  =  \\psi_0 +  \\psi_1 x +  \\psi_2  x +  \\psi_3 x \\\\\n",
    "f(x , \\psi)  =  \\psi_0 + x(\\psi_1  +  \\psi_2   +  \\psi_3) \\\\\n",
    "\n",
    "f(x , \\psi)  =  \\psi_0 + x(\\psi_4)\n",
    "$$Figura 3.3j: Regiones lineales\n",
    "Así, si eliminamos la función de activación tendriamos nuestro primer modelo, que es un modelo lineal. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a9e0d4",
   "metadata": {},
   "source": [
    "**Problema 3.2**  \n",
    "Para cada una de las cuatro regiones lineales en [figura 3.3i](#), indica qué unidades ocultas están inactivas y cuáles están activas (es decir, cuáles recortan y cuáles no recortan sus entradas).\n",
    "\n",
    "**Problema 3.3***  \n",
    "Deriva expresiones para las posiciones de las “uniones” en la función de la [figura 3.3i](#) en términos de los diez parámetros $\\phi$ y de la entrada $x$. Deriva expresiones para las pendientes de las cuatro regiones lineales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d836dc",
   "metadata": {},
   "source": [
    "\n",
    "**Figuras importantes 3.3**\n",
    "\n",
    "\n",
    "\n",
    "![Figura 3.3j: Regiones lineales](figura3.png)\n",
    "\n",
    "**Problema 3.4**  \n",
    "Dibuja una versión de la [figura 3.3](#) donde la intersección en $y$ y la pendiente de la tercera unidad oculta han cambiado como se muestra en la [figura 3.14c](#).  \n",
    "Asume que el resto de los parámetros permanece igual.\n",
    "\n",
    "\n",
    "![Figura 3.3j: Regiones lineales](figura3.14.png)\n",
    "\n",
    "**Figura 3.14**  \n",
    "Procesamiento en una red con una entrada, tres unidades ocultas y una salida para el problema 3.4.  \n",
    "a–c) La entrada para cada unidad oculta es una función lineal de las entradas.  \n",
    "Las dos primeras son iguales a las de la [figura 3.3](#), pero la última es diferente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae46782",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "**Problema 3.5**  \n",
    "Demuestra que la siguiente propiedad se cumple para $\\alpha \\in \\mathbb{R}^+$:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}[\\alpha \\cdot z] = \\alpha \\cdot \\text{ReLU}[z].\n",
    "$$\n",
    "\n",
    "Esto se conoce como la propiedad de *homogeneidad no negativa* de la función ReLU.\n",
    "\n",
    "**Solución**\n",
    "\n",
    "Como $\\alpha$ siempre es positivo, tenemos que la función se corta solo si $z$ es negativo, así por lo tanto ReLU solo depende de $z$. Es importante denotar que esto es falso si $\\alpha$ pudiera ser negativo. \n",
    "\n",
    "---\n",
    "\n",
    "**Problema 3.6**  \n",
    "Continuando con el problema 3.5, ¿qué ocurre con la red neuronal superficial definida en las ecuaciones 3.3 y 3.4 cuando multiplicamos los parámetros $\\theta_{10}$ y $\\theta_{11}$ por una constante positiva $\\alpha$ y dividimos la pendiente $\\phi_1$ por ese mismo parámetro $\\alpha$?  \n",
    "¿Qué sucede si $\\alpha$ es negativo?\n",
    "\n",
    "**Solucionado**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Problema 3.7**  \n",
    "Considera ajustar el modelo en la ecuación 3.1 usando una función de pérdida de mínimos cuadrados.  \n",
    "¿Tiene esta función de pérdida un mínimo único?  \n",
    "Es decir, ¿existe un único conjunto “óptimo” de parámetros?\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Problema 3.8**  \n",
    "Considera reemplazar la función de activación ReLU por:  \n",
    "(i) la función escalón de Heaviside $\\text{heaviside}[z]$,  \n",
    "(ii) la función tangente hiperbólica $\\tanh[z]$, y  \n",
    "(iii) la función rectangular $\\text{rect}[z]$, donde:\n",
    "\n",
    "$$\n",
    "\\text{heaviside}[z] = \\begin{cases} \n",
    "0 & z < 0 \\\\\n",
    "1 & z \\geq 0 \n",
    "\\end{cases}\n",
    "\\quad\\quad\n",
    "\\text{rect}[z] = \\begin{cases} \n",
    "0 & z < 0 \\\\\n",
    "1 & 0 \\leq z \\leq 1 \\\\\n",
    "0 & z > 1 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Dibuja una versión de la [figura 3.3](#) para cada una de estas funciones.  \n",
    "Los parámetros originales eran:  \n",
    "\n",
    "$$\n",
    "\\phi = \\{\\phi_0, \\phi_1, \\phi_2, \\phi_3, \\theta_{10}, \\theta_{11}, \\theta_{20}, \\theta_{21}, \\theta_{30}, \\theta_{31} \\} = \\{-0.23, -1.3, 1.3, 0.66, -0.2, 0.4, -0.9, 0.9, 1.1, -0.7\\}\n",
    "$$\n",
    "\n",
    "Proporciona una descripción informal de la familia de funciones que pueden ser creadas por redes neuronales con una entrada, tres unidades ocultas y una salida para cada función de activación.\n",
    "\n",
    "---\n",
    "\n",
    "**Problema 3.9**  \n",
    "Muestra que la tercera región lineal en la [figura 3.3](#) tiene una pendiente que es la suma de las pendientes de la primera y la cuarta región lineal.\n",
    "\n",
    "---\n",
    "\n",
    "**Problema 3.10**  \n",
    "Considera una red neuronal con una entrada, una salida y tres unidades ocultas.  \n",
    "La construcción en la [figura 3.3](#) muestra cómo se crean cuatro regiones lineales.  \n",
    "¿Bajo qué circunstancias esta red podría producir una función con menos de cuatro regiones lineales?\n",
    "\n",
    "---\n",
    "\n",
    "**Problema 3.11**  \n",
    "¿Cuántos parámetros tiene el modelo en la [figura 3.6](#)?\n",
    "\n",
    "---\n",
    "\n",
    "**Problema 3.12**  \n",
    "¿Cuántos parámetros tiene el modelo en la [figura 3.7](#)?\n",
    "\n",
    "---\n",
    "\n",
    "**Problema 3.13**  \n",
    "¿Cuál es el patrón de activación para cada una de las siete regiones en la [figura 3.8](#)?  \n",
    "En otras palabras, ¿qué unidades ocultas están activas (dejan pasar la entrada) y cuáles están inactivas (recortan la entrada) para cada región?\n",
    "\n",
    "---\n",
    "\n",
    "**Problema 3.14**  \n",
    "Escribe las ecuaciones que definen la red en la [figura 3.11](#).  \n",
    "Debería haber tres ecuaciones para calcular las tres unidades ocultas a partir de las entradas, y dos ecuaciones para calcular las salidas a partir de las unidades ocultas.\n",
    "\n",
    "---\n",
    "\n",
    "**Problema 3.15**  \n",
    "¿Cuál es el número máximo posible de regiones lineales 3D que puede crear la red en la [figura 3.11](#)?\n",
    "\n",
    "---\n",
    "\n",
    "**Problema 3.16**  \n",
    "Escribe las ecuaciones de una red con dos entradas, cuatro unidades ocultas y tres salidas.  \n",
    "Dibuja este modelo al estilo de la [figura 3.11](#).\n",
    "\n",
    "**Solucionado**\n",
    "\n",
    "---\n",
    "\n",
    "**Problema 3.17**  \n",
    "Las ecuaciones 3.11 y 3.12 definen una red neuronal general con $D_i$ entradas, una capa oculta con $D$ unidades ocultas, y $D_o$ salidas.  \n",
    "Encuentra una expresión para el número de parámetros del modelo en términos de $D_i$, $D$ y $D_o$.\n",
    "\n",
    "**Solucionado**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Problema 3.18**  \n",
    "Muestra que el número máximo de regiones creadas por una red superficial con $D_i = 2$ entradas, $D_o = 1$ salida, y $D = 3$ unidades ocultas es siete, como se muestra en la [figura 3.8j](#).  \n",
    "Usa el resultado de [Zaslavsky (1975)](#), que indica que el número máximo de regiones creado al dividir un espacio de dimensión $D_i$ con $D$ hiperplanos es:\n",
    "\n",
    "$$\n",
    "\\sum_{j=0}^{D_i} \\binom{D}{j}\n",
    "$$\n",
    "\n",
    "¿Cuál es el número máximo de regiones si agregamos dos unidades ocultas más a este modelo, es decir, si $D = 5$?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fea1bd",
   "metadata": {},
   "source": [
    "## Preguntas\n",
    "¿Que significaban las derivadas? que significaria la derivada de RELU\n",
    "-  Que significa que las derivadas den 1 o que den el mismo parametro."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
